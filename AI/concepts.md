# Concepts

## 什么是交叉熵

交叉熵（cross-entropy）是一种衡量概率分布之间差异的度量方法。在机器学习中，交叉熵通常用于衡量两个概率分布之间的差异，比如真实标签分布和模型预测分布之间的差异。

具体来说，假设我们有一个分类问题，需要将输入数据分为多个类别，每个类别对应一个概率分布。假设真实标签分布为 $p$，模型预测分布为 $q$，那么它们的交叉熵可以表示为：

$$H(p, q) = -\sum_{i}p(i)\log q(i)$$

其中，$p(i)$ 表示真实标签分布中第 $i$ 个类别的概率，$q(i)$ 表示模型预测分布中第 $i$ 个类别的概率。交叉熵越小，表示两个分布之间越接近，模型的预测效果也越好。

在深度学习中，交叉熵通常被用作损失函数，用于衡量模型预测与真实标签之间的差异，并通过反向传播算法来更新模型的参数。

## 什么是似然函数

似然函数（likelihood function）是统计学中用于描述概率模型的函数。似然函数通常用于从数据中估计模型参数。

具体来说，假设我们有一组数据 $X$，并假设这些数据服从某种概率分布，但是该分布的参数未知。我们希望从这些数据中推断出该分布的参数。假设我们有一个候选参数 $\theta$，我们可以计算在给定该参数的条件下，数据集 $X$ 出现的概率，这个概率称为似然函数。似然函数通常表示为 $L(\theta | X)$，其中 $\theta$ 是分布的参数，$X$是观测数据。

具体来说，对于连续型分布，似然函数可以表示为：

$$L(\theta | X) = f_{\theta}(x_1)f_{\theta}(x_2) \dots f_{\theta}(x_n)$$

其中，$f_{\theta}(x_i)$ 表示在给定参数 $\theta$ 的情况下，观测数据 $x_i$ 出现的概率密度函数。对于离散型分布，似然函数可以表示为：

$$L(\theta | X) = \prod_{i=1}^{n} f_{\theta}(x_i)$$

其中，$f_{\theta}(x_i)$ 表示在给定参数 $\theta$ 的情况下，观测数据 $x_i$ 出现的概率质量函数。

在实际应用中，我们通常希望最大化似然函数，即找到最优的参数 $\theta$，使得似然函数取最大值。这个过程称为最大似然估计。最大似然估计是一种常见的参数估计方法，在许多机器学习和统计模型中都得到广泛应用。