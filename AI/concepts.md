# Concepts

## 什么是交叉熵

交叉熵（cross-entropy）是一种衡量概率分布之间差异的度量方法。在机器学习中，交叉熵通常用于衡量两个概率分布之间的差异，比如真实标签分布和模型预测分布之间的差异。

具体来说，假设我们有一个分类问题，需要将输入数据分为多个类别，每个类别对应一个概率分布。假设真实标签分布为 $p$，模型预测分布为 $q$，那么它们的交叉熵可以表示为：

$$H(p, q) = -\sum_{i}p(i)\log q(i)$$

其中，$p(i)$ 表示真实标签分布中第 $i$ 个类别的概率，$q(i)$ 表示模型预测分布中第 $i$ 个类别的概率。交叉熵越小，表示两个分布之间越接近，模型的预测效果也越好。

在深度学习中，交叉熵通常被用作损失函数，用于衡量模型预测与真实标签之间的差异，并通过反向传播算法来更新模型的参数。